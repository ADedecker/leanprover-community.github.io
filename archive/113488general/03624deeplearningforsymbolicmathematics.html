---
layout: archive
title: Lean Prover Zulip Chat Archive
permalink: archive/113488general/03624deeplearningforsymbolicmathematics.html
---

<h2>Stream: <a href="https://leanprover-community.github.io/archive/113488general/index.html">general</a>
<h3>Topic: <a href="https://leanprover-community.github.io/archive/113488general/03624deeplearningforsymbolicmathematics.html">deep learning for symbolic mathematics</a></h3>

<hr>

<base href="https://leanprover.zulipchat.com">

{% raw %}
<a name="185657086"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/deep%20learning%20for%20symbolic%20mathematics/near/185657086" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/03624deeplearningforsymbolicmathematics.html#185657086">Tim Daly (Jan 15 2020 at 00:39)</a>:</h4>
<p>Lample Guillaume and Charton, Francois "Deep Learning for Symbolic Mathematics"  (<a href="https://arxiv.org/pdf/1912.01412.pdf" target="_blank" title="https://arxiv.org/pdf/1912.01412.pdf">https://arxiv.org/pdf/1912.01412.pdf</a></p>

<a name="185657756"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/deep%20learning%20for%20symbolic%20mathematics/near/185657756" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/03624deeplearningforsymbolicmathematics.html#185657756">Tim Daly (Jan 15 2020 at 00:52)</a>:</h4>
<p>I didn't think this was possible and I was wrong. Now the question is, can similar techniques apply to Lean? They did have the advantage that they could generate large datasets because they could generate a random equation, differentiate it, and then know that the integral exists. I'm not sure how to generate "random" proofs.</p>

<a name="185658084"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/deep%20learning%20for%20symbolic%20mathematics/near/185658084" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/03624deeplearningforsymbolicmathematics.html#185658084">Tim Daly (Jan 15 2020 at 00:59)</a>:</h4>
<p>Their technique of Integerate(Diff(f(x),x)) also generates a lot of random test cases (fuzzing) useful for debugging. So if there was a similar hack for Lean then one could build a fuzz tester for Lean.</p>
<p>The only thing I can think of is there is the idea of "generate all proofs of length N" (usually used for the idea of exhaustive search). I don't know how to generate proofs of length N though.</p>

<a name="185658500"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/deep%20learning%20for%20symbolic%20mathematics/near/185658500" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/03624deeplearningforsymbolicmathematics.html#185658500">Simon Cruanes (Jan 15 2020 at 01:07)</a>:</h4>
<p>If you consider the set of rules of some sequent calculus (like HOL's kernel), you can use them bottom-to-top as generators as if they were prolog rules, I think.<br>
From</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">Γ</mi><mo separator="true">,</mo><mi>A</mi><mo>⊢</mo><mi>B</mi></mrow><mrow><mi mathvariant="normal">Γ</mi><mo>⊢</mo><mi>A</mi><mo>⇒</mo><mi>B</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\cfrac{\Gamma, A \vdash B}{\Gamma \vdash A \Rightarrow B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.276em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5899999999999999em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">Γ</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">⊢</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault">A</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">⇒</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">Γ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">A</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">⊢</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span></span></span></span></span></span></span></p>
<p>you can start with a formula <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>φ</mi></mrow><annotation encoding="application/x-tex">\varphi</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">φ</span></span></span></span> and unify it with</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>⇒</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">A \Rightarrow B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">A</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">⇒</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span></span></span></span></span></p>
<p>and recursively unify <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">A</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span></span></span></span> with the conclusions of other rules?<br>
I know it's doable at least for generating terms with ML-style polymorphic types, I've used that in the past for quickcheck stuff.</p>
<p>For Lean you'd run the refiner "in reverse", in a similar way.</p>

<a name="185658846"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/deep%20learning%20for%20symbolic%20mathematics/near/185658846" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/03624deeplearningforsymbolicmathematics.html#185658846">Tim Daly (Jan 15 2020 at 01:14)</a>:</h4>
<p>In the paper they create a tree structure by moving the "operator" to the prefix position (e.g. 2 + 3 becomes (+ 2 3). So A \Rightarrow B becomes (\Rightarrow A B) in their generator. It should be possible to create a generator using that tree-method.</p>

<a name="185659173"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/deep%20learning%20for%20symbolic%20mathematics/near/185659173" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/03624deeplearningforsymbolicmathematics.html#185659173">Tim Daly (Jan 15 2020 at 01:20)</a>:</h4>
<p>The next hack would be to have a set of tactics that were exactly the rules in Lean. Then, when you generate a theorem (in the forward direction by creating a proof tree) you can look at the theorem and apply the tactics to reconstruct the proof.  Since you know the theorem is true, and you know the tactics cover the set of rules used to generate the proof, Lean should be able to (automatically?) prove the theorem.<br>
Maybe. Possibly.</p>

<a name="185659289"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/deep%20learning%20for%20symbolic%20mathematics/near/185659289" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/03624deeplearningforsymbolicmathematics.html#185659289">Tim Daly (Jan 15 2020 at 01:23)</a>:</h4>
<p>Once you have a large enough training set then perhaps Lample and Charton can run their ML program on it and "learn" to prove.</p>


{% endraw %}

{% include archive_update.html %}