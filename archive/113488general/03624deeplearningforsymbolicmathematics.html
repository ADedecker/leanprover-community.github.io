---
layout: archive
title: Lean Prover Zulip Chat Archive
permalink: archive/113488general/03624deeplearningforsymbolicmathematics.html
---

<h2>Stream: <a href="https://leanprover-community.github.io/archive/113488general/index.html">general</a>
<h3>Topic: <a href="https://leanprover-community.github.io/archive/113488general/03624deeplearningforsymbolicmathematics.html">deep learning for symbolic mathematics</a></h3>

<hr>

<base href="https://leanprover.zulipchat.com">

{% raw %}
<a name="185657086"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/deep%20learning%20for%20symbolic%20mathematics/near/185657086" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/03624deeplearningforsymbolicmathematics.html#185657086">Tim Daly (Jan 15 2020 at 00:39)</a>:</h4>
<p>Lample Guillaume and Charton, Francois "Deep Learning for Symbolic Mathematics"  (<a href="https://arxiv.org/pdf/1912.01412.pdf" target="_blank" title="https://arxiv.org/pdf/1912.01412.pdf">https://arxiv.org/pdf/1912.01412.pdf</a></p>

<a name="185657756"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/deep%20learning%20for%20symbolic%20mathematics/near/185657756" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/03624deeplearningforsymbolicmathematics.html#185657756">Tim Daly (Jan 15 2020 at 00:52)</a>:</h4>
<p>I didn't think this was possible and I was wrong. Now the question is, can similar techniques apply to Lean? They did have the advantage that they could generate large datasets because they could generate a random equation, differentiate it, and then know that the integral exists. I'm not sure how to generate "random" proofs.</p>

<a name="185658084"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/deep%20learning%20for%20symbolic%20mathematics/near/185658084" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/113488general/03624deeplearningforsymbolicmathematics.html#185658084">Tim Daly (Jan 15 2020 at 00:59)</a>:</h4>
<p>Their technique of Integerate(Diff(f(x),x)) also generates a lot of random test cases (fuzzing) useful for debugging. So if there was a similar hack for Lean then one could build a fuzz tester for Lean.</p>
<p>The only thing I can think of is there is the idea of "generate all proofs of length N" (usually used for the idea of exhaustive search). I don't know how to generate proofs of length N though.</p>


{% endraw %}

{% include archive_update.html %}