---
layout: archive
title: Lean Prover Zulip Chat Archive
permalink: archive/219941MachineLearningforTheoremProving/34292HOList.html
---

<h2>Stream: <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/index.html">Machine Learning for Theorem Proving</a>
<h3>Topic: <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html">HOList</a></h3>

<hr>

<base href="https://leanprover.zulipchat.com">

{% raw %}
<a name="185555422"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185555422" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185555422">Jason Rute (Jan 14 2020 at 01:01)</a>:</h4>
<p><a href="https://sites.google.com/view/holist/home" target="_blank" title="https://sites.google.com/view/holist/home">DeepHOL/HOList</a> is Google Research neural-based automatic theorem prover for HOL Light.  A number of us have had deep discussions about it, how it works, and some of its design decisions.  Here are some highlights:</p>
<ul>
<li><span class="user-mention" data-user-id="213234">@Aaron Hadley</span>  and his team at UCF have made a great notebook demonstrating the "front end" Python API how to extend HOList to use other machine learning models <a href="https://github.com/aahadley/deepmath-jupyter/blob/master/HOLJup.ipynb" target="_blank" title="https://github.com/aahadley/deepmath-jupyter/blob/master/HOLJup.ipynb">here</a> and they also added a <a href="https://github.com/aahadley/deepmath-jupyter/blob/master/TutorialPaper.pdf" target="_blank" title="https://github.com/aahadley/deepmath-jupyter/blob/master/TutorialPaper.pdf">tutorial</a>.</li>
<li>If you are more interested in how DeepHOL (the neural prover) communicates with HOList (the modified from of HOL Light), here is a <a href="https://github.com/jasonrute/holist-communication-example" target="_blank" title="https://github.com/jasonrute/holist-communication-example">project of mine</a> which fleshes out the backend API.  In particular, <a href="https://github.com/jasonrute/holist-communication-example/blob/master/walkthrough_of_holist_api.ipynb" target="_blank" title="https://github.com/jasonrute/holist-communication-example/blob/master/walkthrough_of_holist_api.ipynb">this notebook</a> walks you through the gRPC API.</li>
<li>It should be noted that a lot of the backend API is not used in the front end API.  For example, currently DeepHOL can't supply term parameters to tactics.  But it can choose tactics and choose theorem parameters (premise selection).</li>
<li>If anyone is interesting in hooking up Lean (or any other ITP) to DeepHOL, <a href="https://gist.github.com/jasonrute/00109af2bdc0974d2e8e79faf26ba556" target="_blank" title="https://gist.github.com/jasonrute/00109af2bdc0974d2e8e79faf26ba556">here</a> is a very preliminary best guess at what it would take to do it after talking with <span class="user-mention" data-user-id="217806">@Markus Rabe</span> at Google.</li>
</ul>

<a name="185555480"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185555480" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185555480">Jason Rute (Jan 14 2020 at 01:02)</a>:</h4>
<p>I know there are still a lot of questions about HOList (like why it uses s-expressions).  Feel free to discuss here.</p>

<a name="185575109"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185575109" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185575109">Stanislas Polu (Jan 14 2020 at 08:49)</a>:</h4>
<p>On the question of S-expression, I think that we went to the bottom of it through various private discussions. Pretty-printed HOL-light expressions which are appealing because they are close to what a human formalizing a proof would use are unfortunately ambiguous. The parser supports type annotation for human to disambiguate term types when coding in HOL Light, but unfortunately the pretty-printer is destructive such that <code>pretty_print o parse</code> is not the identity.</p>
<p>S-expressions are unambiguous as they are a natural way to marshal the in-memory representation of HOL Light terms (with every variable/constant being explicitly typed).</p>
<p>Ideally for some ML application, having compact representations is useful. Theoretically we could record the top-level semi-typed parsable theorem and term expressions and have them appear in proof logs as tactics arguments but that's probably not practical because hol-light starts by turning these expressions into in-memory representations, destroying the disambiguated human-provided terms.</p>
<p>Hope this context is useful!</p>

<a name="185593684"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185593684" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185593684">Jason Rute (Jan 14 2020 at 13:07)</a>:</h4>
<p>I think to take a step back, we have to acknowledge that s-expressions are used in 2.5 different ways in HOList.</p>
<ul>
<li>They are used as a serialization method to send HOL Light terms back and forth between HOList and DeepHOL (again, see my <a href="https://github.com/jasonrute/holist-communication-example/blob/master/walkthrough_of_holist_api.ipynb" target="_blank" title="https://github.com/jasonrute/holist-communication-example/blob/master/walkthrough_of_holist_api.ipynb">"backend" walkthrough of HOList/DeepHOL</a>.)  This is sort of natural since the terms are algebraic data structures and s-expressions are natural for capturing such algebraic data structures.  This would generalize to any formal logic I can think of, making it sort of language neutral (before training).  However, I can also imagine one could use other serialization methods.  One could have used JSON or gRPC (which naturally translates to JSON) as the serialization method as well.  <span class="user-mention" data-user-id="246156">@Brando Miranda</span> has asked about JSON and said that Emilio at <a href="https://github.com/ejgallego/coq-serapi" target="_blank" title="https://github.com/ejgallego/coq-serapi">Serapi/Coq</a> was thinking about switching from s-expressions to JSON.  In some sense I don't think it matters much here as long as one uses a lossless encoding which is easy to parse.  (And in this case, these s-expressions are extremely easy to parse into whatever form one wants.)</li>
<li>They are also used as the text entered into the neural network model.  If one is using a pure sequence input model like an RNN or wavenet, then I think the idea is that you would plug the s-expression in as is (except one-hot encoding every token first?).  Or if one was using a tree or graph NN, then one naturally parses the s-expression into that tree or graph.  <span class="user-mention" data-user-id="249373">@Stanislas Polu</span> has lamented that the current s-expressions are probably too long for an RNN and has suggested shorter encodings.  Again, I think one has full freedom to play around with other representations.  I can think of many.  On the most compact side is to use the compact representation from the HOLStep data set.  It throws away types.  It uses polish notation.  And it uses skolemization and DeBruin indices for quantifiers and variables.  This however might be too compact.  HOList has found (from taking with Markus) that variable names matter a lot.  Another compact-ish approach would be to simulate the HOL Light pretty printer, but maybe add a few extra things.  It wouldn't be hard to get the parentheses the same as HOL Light.  As for types, the HOL Light pretty printer throws them away, but I think one might want to keep them for quantifiers and lambdas only.  Now, intermediate goals might not have any quantifiers, but one could borrow notation from say Lean and write something like this for an intermediate goal <code>(n: nat), (m: nat) |- = (num_add m n) (num_add n m)</code>.  It is hard to know what would work best without experimentation.  I've suggested that rather than trying this all out on HOList, it might be better to experiment first with some of the different string representations with different neural network models on HOLStep first since it is an easier to train dataset.  We would be looking for something which is quick to run but also does well on the task.  Since it is so up in the air what a good string input is, I think it wouldn't make sense to encode it into the HOList/DeepMath interface.  Instead, s-expressions work nice as a loss-less encoding which could be tweaked into something else when needed.</li>
<li>Last, one is also using these s-expressions as some sort of semi-human-readable term representation.  This is needed for debugging and understanding the outputs.  Again, the programmer is free to clean up the formula a bit for debugging, but most of the current HOList printouts use these s-expressions.</li>
</ul>
<p>So in summary, s-expressions try to be everything to everyone.  While in practice they might fail at that, they are pretty easy to parse and turn into something else.  The only exception is that the HOL Light pretty printed expressions are a bit complicated to 100% reproduce, but one can come close.  If it is important to have the exact pretty printed expressions, one could build that as another server call into the gRPC interface.  Get me the pretty printed version of this s-expression.</p>

<a name="185607167"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185607167" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185607167">Markus Rabe (Jan 14 2020 at 15:36)</a>:</h4>
<p>The response to any apply tactic request should already include the pretty printed version of the terms (as well as the s-expression, of course). Also all the theorem in the theorem database should have a pretty printed field in their proto.</p>

<a name="185624371"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185624371" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185624371">Christian Szegedy (Jan 14 2020 at 18:29)</a>:</h4>
<p>Our first models were convolutional networks that take the  tokenized s-expression (with types) as input. We have removed the parenthesis as well, as it is redundant, but makes parsing of the expressions a bit easier (esp. for humans). </p>
<p>We have tried to train sequence models on the pretty printed output as well, but it yielded inferior results to the models taking s-expressions as input.</p>
<p>Our graph-neural networks uses subexpression-sharing (still containing) which makes the input significantly shorter</p>
<p>Using JSON would have had the disadvantage that we would have needed a JSON parser in our Google internal version of HOL Light, which would have required importing extra packages.</p>

<a name="185655886"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185655886" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185655886">Jason Rute (Jan 15 2020 at 00:17)</a>:</h4>
<blockquote>
<p>The response to any apply tactic request should already include the pretty printed version of the terms </p>
</blockquote>
<p>I didn’t see this behavior in <a href="https://github.com/jasonrute/holist-communication-example/blob/master/walkthrough_of_holist_api.ipynb" target="_blank" title="https://github.com/jasonrute/holist-communication-example/blob/master/walkthrough_of_holist_api.ipynb">my notebook</a>.  All the apply tactic calls only return the full s-expressions.  For example see cell 10.</p>

<a name="185746134"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185746134" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185746134">Patrick Massot (Jan 15 2020 at 21:28)</a>:</h4>
<p><span class="user-mention" data-user-id="110026">@Simon Hudon</span> are you following this thread in order to see how the Lean4 editor integration could also be a machine learning rig integration? Or is it something completely different?</p>

<a name="185746790"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185746790" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185746790">Jesse Michael Han (Jan 15 2020 at 21:35)</a>:</h4>
<p>such integration would be more structured than the current language server protocol, which does not serialize the environment, nor fully elaborated terms (and only does so as unstructured strings via JSON)</p>

<a name="185746915"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185746915" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185746915">Simon Hudon (Jan 15 2020 at 21:36)</a>:</h4>
<p>The next language server will serialize the syntax tree and the type information</p>

<a name="185747146"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185747146" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185747146">Simon Hudon (Jan 15 2020 at 21:38)</a>:</h4>
<p><span class="user-mention" data-user-id="110031">@Patrick Massot</span>, does that answer your question? I'm not sure I understood what you were looking for</p>

<a name="185747174"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185747174" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185747174">Simon Cruanes (Jan 15 2020 at 21:39)</a>:</h4>
<p>So it'll still be bespoke and not LSP based?</p>

<a name="185747579"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185747579" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185747579">Patrick Massot (Jan 15 2020 at 21:43)</a>:</h4>
<blockquote>
<p>So it'll still be bespoke and not LSP based?</p>
</blockquote>
<p>We can add as many extension to LSP as we want.</p>

<a name="185747586"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185747586" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185747586">Simon Hudon (Jan 15 2020 at 21:43)</a>:</h4>
<p>We're basing it on LSP but we're going to get beyond the LSP basic features for the more advance uses</p>

<a name="185747611"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185747611" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185747611">Patrick Massot (Jan 15 2020 at 21:43)</a>:</h4>
<blockquote>
<p>Patrick Massot, does that answer your question? I'm not sure I understood what you were looking for</p>
</blockquote>
<p>I have no idea. I only hope people who followed this thread will know.</p>

<a name="185747612"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185747612" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185747612">Mario Carneiro (Jan 15 2020 at 21:43)</a>:</h4>
<p>I'm hoping you document those extensions</p>

<a name="185747736"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185747736" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185747736">Simon Cruanes (Jan 15 2020 at 21:45)</a>:</h4>
<blockquote>
<p>We can add as many extension to LSP as we want.</p>
</blockquote>
<p>yes, but LSP remains based on buffers and JSON, I'm not sure I see how you can carry ASTs on it efficiently?</p>

<a name="185747750"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185747750" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185747750">Mario Carneiro (Jan 15 2020 at 21:45)</a>:</h4>
<p>you can json anything</p>

<a name="185747804"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185747804" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185747804">Mario Carneiro (Jan 15 2020 at 21:46)</a>:</h4>
<p>I guess the efficiency isn't so great, but as long as it's only sent when needed it shouldn't be so bad</p>

<a name="185747855"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185747855" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185747855">Simon Cruanes (Jan 15 2020 at 21:46)</a>:</h4>
<p>sometimes I wish LSP had been built on msgpack-rpc or something like that</p>

<a name="185748765"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185748765" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185748765">Mario Carneiro (Jan 15 2020 at 21:58)</a>:</h4>
<p>huh, msgpack is pretty cool</p>

<a name="185749163"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185749163" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185749163">Simon Cruanes (Jan 15 2020 at 22:03)</a>:</h4>
<p>especially when you want to embed big chunks of code into it… no escaping needed</p>

<a name="186201897"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186201897" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186201897">Stanislas Polu (Jan 21 2020 at 16:46)</a>:</h4>
<p><span class="user-mention" data-user-id="115715">@Jason Rute</span> and others friends knowledgeable about Holist. I've been able to spin up a holist instance using the <code>gcr.io/deepmath/hol-light</code> image. One thing that I expected from reading the code and introspecting the proof logs was that all theorems fingerprints appearing in the proof logs would be directly usable with that image but it looks like that's not the case. How does one is supposed to interact with the prover for a goal part of the test set? Do they have to replay and register all theorems first?</p>

<a name="186202542"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186202542" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186202542">Stanislas Polu (Jan 21 2020 at 16:53)</a>:</h4>
<p>Example code failing:</p>
<div class="codehilite"><pre><span></span>for_all_x_exists_y_x_equals_y = Theorem(
    name=&quot;FORALL_X_EXISTS_Y_SUCH_THAT_X_EQUALS_Y&quot;,
    conclusion=&quot;(a (c (fun (fun A (bool)) (bool)) !) (l (v A x) (a (c (fun (fun A (bool)) (bool)) ?) (l (v A y) (a (a (c (fun A (fun A (bool))) =) (v A x)) (v A y))))))&quot;,
    training_split=Theorem.Split.TESTING,
    tag=Theorem.Tag.THEOREM,
)

if __name__ == &#39;__main__&#39;:
    with grpc.insecure_channel(&#39;10.72.7.138:2000&#39;) as channel:
        stub = ProofAssistantServiceStub(channel)

        request3 = ApplyTacticRequest(goal=for_all_x_exists_y_x_equals_y, tactic=&quot;SIMP_TAC [ THM 220805353555668225 ]&quot;)
        print(&quot;Request:&quot;)
        print(request3)

        response3 = stub.ApplyTactic(request3)
        print(&quot;Response:&quot;)
        print(response3)
</pre></div>


<p>Where <code>220805353555668225</code> is the fingerprint of a theorem argument taken from the training set (appears in <code>human/train/prooflogs-00037-of-00600.pbtxt</code>)</p>
<p>This gives:</p>
<div class="codehilite"><pre><span></span>Request:
goal {
  conclusion: &quot;(a (c (fun (fun A (bool)) (bool)) !) (l (v A x) (a (c (fun (fun A (bool)) (bool)) ?) (l (v A y) (a (a (c (fun A (fun A (bool))) =) (v A x)) (v A y))))))&quot;
  tag: THEOREM
  name: &quot;FORALL_X_EXISTS_Y_SUCH_THAT_X_EQUALS_Y&quot;
  training_split: TESTING
}
tactic: &quot;SIMP_TAC [ THM 220805353555668225 ]&quot;

Response:
error: &quot;Failure(\&quot;No theorem exists with index 220805353555668225\&quot;)&quot;
</pre></div>

<a name="186202862"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186202862" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186202862">Stanislas Polu (Jan 21 2020 at 16:56)</a>:</h4>
<p>Or in other words how can I easily replay a proof log ?</p>

<a name="186203965"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186203965" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186203965">Stanislas Polu (Jan 21 2020 at 17:06)</a>:</h4>
<p>Ah I now realize that some theorems in <code>theorem_database_v1.1.textpb</code> are registered and usable.  I think only definitions are registered, but other theorems are not. </p>
<p>The question therefore remains?</p>

<a name="186210355"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186210355" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186210355">Jason Rute (Jan 21 2020 at 18:10)</a>:</h4>
<p>I was under the impression, possibly wrong, that one needs to replay (VerifyProof) and register (RegisterTheorem) all the theorems.  Of course this is assuming you are working in the “low level” gRPC API.  If you are working in the “high level” Python API then I assume the Python code does that stuff for you, but I haven’t explored that as much yet.</p>

<a name="186211833"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186211833" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186211833">Stanislas Polu (Jan 21 2020 at 18:27)</a>:</h4>
<p>Maybe <span class="user-mention" data-user-id="217806">@Markus Rabe</span> or <span class="user-mention" data-user-id="239426">@Christian Szegedy</span> can shed some light on this? <span aria-label="grimacing" class="emoji emoji-1f62c" role="img" title="grimacing">:grimacing:</span></p>

<a name="186218006"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186218006" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186218006">Christian Szegedy (Jan 21 2020 at 19:31)</a>:</h4>
<blockquote>
<blockquote>
<p>The response to any apply tactic request should already include the pretty printed version of the terms </p>
</blockquote>
<p>I didn’t see this behavior in <a href="https://github.com/jasonrute/holist-communication-example/blob/master/walkthrough_of_holist_api.ipynb" target="_blank" title="https://github.com/jasonrute/holist-communication-example/blob/master/walkthrough_of_holist_api.ipynb">my notebook</a>.  All the apply tactic calls only return the full s-expressions.  For example see cell 10.</p>
</blockquote>
<p>It looks like (unfortunately) that this is only in our Google-internal version. We could do another round of exporting, especially that we have some code for an ICLR paper that should be open-sourced as well.</p>

<a name="186218426"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186218426" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186218426">Christian Szegedy (Jan 21 2020 at 19:35)</a>:</h4>
<blockquote>
<p>I was under the impression, possibly wrong, that one needs to replay (VerifyProof) and register (RegisterTheorem) all the theorems.  Of course this is assuming you are working in the “low level” gRPC API.  If you are working in the “high level” Python API then I assume the Python code does that stuff for you, but I haven’t explored that as much yet.</p>
</blockquote>
<p>The current verifier verifies theorems in their original context. So currently. you need to replay the whole theorem library in order to verify any number of theorems:<br>
- You can verify only theorems that came from the HOL-Light library (complex),<br>
- You can verify any number of theorems (you don't need to verify all of them)<br>
- All the theorems in the library will be run through the kernel for verification.<br>
- Those theorems that had an external proof (to be verified) will use their external proof at exactly that position where the theorem was proved originally.</p>

<a name="186220118"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186220118" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186220118">Christian Szegedy (Jan 21 2020 at 19:52)</a>:</h4>
<blockquote>
<p>Ah I now realize that some theorems in <code>theorem_database_v1.1.textpb</code> are registered and usable.  I think only definitions are registered, but other theorems are not. </p>
<p>The question therefore remains?</p>
</blockquote>
<p>The human prooflogs contain proof-steps that rely on theorems created "on the fly" by forward reasoning steps (so called conversions). These theorems don't show up in the theorem database. Also proofs relying on them can't be replayed as conversions can't be replayed either.</p>
<p>On the other hand the exported tensorflow examples contain the actual s-expression of those parameters, so if somebody uses those examples, the corresponding theorem can be used for training the parameter-selection models, even if those tactic-parameters don't show up in the proof-logs.</p>
<p>Around 60% of the human proofs can be replayed, as a large number of them uses theorems deduced by forward reasoning steps, other theorems use ad-hoc substitution of terms that we did not log either.</p>


{% endraw %}

{% include archive_update.html %}