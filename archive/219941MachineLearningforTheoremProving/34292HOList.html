---
layout: archive
title: Lean Prover Zulip Chat Archive
permalink: archive/219941MachineLearningforTheoremProving/34292HOList.html
---

<h2>Stream: <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/index.html">Machine Learning for Theorem Proving</a>
<h3>Topic: <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html">HOList</a></h3>

<hr>

<base href="https://leanprover.zulipchat.com">

{% raw %}
<a name="185555422"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185555422" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185555422">Jason Rute (Jan 14 2020 at 01:01)</a>:</h4>
<p><a href="https://sites.google.com/view/holist/home" target="_blank" title="https://sites.google.com/view/holist/home">DeepHOL/HOList</a> is Google Research neural-based automatic theorem prover for HOL Light.  A number of us have had deep discussions about it, how it works, and some of its design decisions.  Here are some highlights:</p>
<ul>
<li><span class="user-mention" data-user-id="213234">@Aaron Hadley</span>  and his team at UCF have made a great notebook demonstrating the "front end" Python API how to extend HOList to use other machine learning models <a href="https://github.com/aahadley/deepmath-jupyter/blob/master/HOLJup.ipynb" target="_blank" title="https://github.com/aahadley/deepmath-jupyter/blob/master/HOLJup.ipynb">here</a> and they also added a <a href="https://github.com/aahadley/deepmath-jupyter/blob/master/TutorialPaper.pdf" target="_blank" title="https://github.com/aahadley/deepmath-jupyter/blob/master/TutorialPaper.pdf">tutorial</a>.</li>
<li>If you are more interested in how DeepHOL (the neural prover) communicates with HOList (the modified from of HOL Light), here is a <a href="https://github.com/jasonrute/holist-communication-example" target="_blank" title="https://github.com/jasonrute/holist-communication-example">project of mine</a> which fleshes out the backend API.  In particular, <a href="https://github.com/jasonrute/holist-communication-example/blob/master/walkthrough_of_holist_api.ipynb" target="_blank" title="https://github.com/jasonrute/holist-communication-example/blob/master/walkthrough_of_holist_api.ipynb">this notebook</a> walks you through the gRPC API.</li>
<li>It should be noted that a lot of the backend API is not used in the front end API.  For example, currently DeepHOL can't supply term parameters to tactics.  But it can choose tactics and choose theorem parameters (premise selection).</li>
<li>If anyone is interesting in hooking up Lean (or any other ITP) to DeepHOL, <a href="https://gist.github.com/jasonrute/00109af2bdc0974d2e8e79faf26ba556" target="_blank" title="https://gist.github.com/jasonrute/00109af2bdc0974d2e8e79faf26ba556">here</a> is a very preliminary best guess at what it would take to do it after talking with <span class="user-mention" data-user-id="217806">@Markus Rabe</span> at Google.</li>
</ul>

<a name="185555480"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185555480" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185555480">Jason Rute (Jan 14 2020 at 01:02)</a>:</h4>
<p>I know there are still a lot of questions about HOList (like why it uses s-expressions).  Feel free to discuss here.</p>

<a name="185575109"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185575109" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185575109">Stanislas Polu (Jan 14 2020 at 08:49)</a>:</h4>
<p>On the question of S-expression, I think that we went to the bottom of it through various private discussions. Pretty-printed HOL-light expressions which are appealing because they are close to what a human formalizing a proof would use are unfortunately ambiguous. The parser supports type annotation for human to disambiguate term types when coding in HOL Light, but unfortunately the pretty-printer is destructive such that <code>pretty_print o parse</code> is not the identity.</p>
<p>S-expressions are unambiguous as they are a natural way to marshal the in-memory representation of HOL Light terms (with every variable/constant being explicitly typed).</p>
<p>Ideally for some ML application, having compact representations is useful. Theoretically we could record the top-level semi-typed parsable theorem and term expressions and have them appear in proof logs as tactics arguments but that's probably not practical because hol-light starts by turning these expressions into in-memory representations, destroying the disambiguated human-provided terms.</p>
<p>Hope this context is useful!</p>


{% endraw %}

{% include archive_update.html %}