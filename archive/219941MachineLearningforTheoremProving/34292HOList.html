---
layout: archive
title: Lean Prover Zulip Chat Archive
permalink: archive/219941MachineLearningforTheoremProving/34292HOList.html
---

<h2>Stream: <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/index.html">Machine Learning for Theorem Proving</a>
<h3>Topic: <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html">HOList</a></h3>

<hr>

<base href="https://leanprover.zulipchat.com">

{% raw %}
<a name="185555422"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185555422" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185555422">Jason Rute (Jan 14 2020 at 01:01)</a>:</h4>
<p><a href="https://sites.google.com/view/holist/home" target="_blank" title="https://sites.google.com/view/holist/home">DeepHOL/HOList</a> is Google Research neural-based automatic theorem prover for HOL Light.  A number of us have had deep discussions about it, how it works, and some of its design decisions.  Here are some highlights:</p>
<ul>
<li><span class="user-mention" data-user-id="213234">@Aaron Hadley</span>  and his team at UCF have made a great notebook demonstrating the "front end" Python API how to extend HOList to use other machine learning models <a href="https://github.com/aahadley/deepmath-jupyter/blob/master/HOLJup.ipynb" target="_blank" title="https://github.com/aahadley/deepmath-jupyter/blob/master/HOLJup.ipynb">here</a> and they also added a <a href="https://github.com/aahadley/deepmath-jupyter/blob/master/TutorialPaper.pdf" target="_blank" title="https://github.com/aahadley/deepmath-jupyter/blob/master/TutorialPaper.pdf">tutorial</a>.</li>
<li>If you are more interested in how DeepHOL (the neural prover) communicates with HOList (the modified from of HOL Light), here is a <a href="https://github.com/jasonrute/holist-communication-example" target="_blank" title="https://github.com/jasonrute/holist-communication-example">project of mine</a> which fleshes out the backend API.  In particular, <a href="https://github.com/jasonrute/holist-communication-example/blob/master/walkthrough_of_holist_api.ipynb" target="_blank" title="https://github.com/jasonrute/holist-communication-example/blob/master/walkthrough_of_holist_api.ipynb">this notebook</a> walks you through the gRPC API.</li>
<li>It should be noted that a lot of the backend API is not used in the front end API.  For example, currently DeepHOL can't supply term parameters to tactics.  But it can choose tactics and choose theorem parameters (premise selection).</li>
<li>If anyone is interesting in hooking up Lean (or any other ITP) to DeepHOL, <a href="https://gist.github.com/jasonrute/00109af2bdc0974d2e8e79faf26ba556" target="_blank" title="https://gist.github.com/jasonrute/00109af2bdc0974d2e8e79faf26ba556">here</a> is a very preliminary best guess at what it would take to do it after talking with <span class="user-mention" data-user-id="217806">@Markus Rabe</span> at Google.</li>
</ul>

<a name="185555480"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185555480" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185555480">Jason Rute (Jan 14 2020 at 01:02)</a>:</h4>
<p>I know there are still a lot of questions about HOList (like why it uses s-expressions).  Feel free to discuss here.</p>

<a name="185575109"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185575109" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185575109">Stanislas Polu (Jan 14 2020 at 08:49)</a>:</h4>
<p>On the question of S-expression, I think that we went to the bottom of it through various private discussions. Pretty-printed HOL-light expressions which are appealing because they are close to what a human formalizing a proof would use are unfortunately ambiguous. The parser supports type annotation for human to disambiguate term types when coding in HOL Light, but unfortunately the pretty-printer is destructive such that <code>pretty_print o parse</code> is not the identity.</p>
<p>S-expressions are unambiguous as they are a natural way to marshal the in-memory representation of HOL Light terms (with every variable/constant being explicitly typed).</p>
<p>Ideally for some ML application, having compact representations is useful. Theoretically we could record the top-level semi-typed parsable theorem and term expressions and have them appear in proof logs as tactics arguments but that's probably not practical because hol-light starts by turning these expressions into in-memory representations, destroying the disambiguated human-provided terms.</p>
<p>Hope this context is useful!</p>

<a name="185593684"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185593684" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185593684">Jason Rute (Jan 14 2020 at 13:07)</a>:</h4>
<p>I think to take a step back, we have to acknowledge that s-expressions are used in 2.5 different ways in HOList.</p>
<ul>
<li>They are used as a serialization method to send HOL Light terms back and forth between HOList and DeepHOL (again, see my <a href="https://github.com/jasonrute/holist-communication-example/blob/master/walkthrough_of_holist_api.ipynb" target="_blank" title="https://github.com/jasonrute/holist-communication-example/blob/master/walkthrough_of_holist_api.ipynb">"backend" walkthrough of HOList/DeepHOL</a>.)  This is sort of natural since the terms are algebraic data structures and s-expressions are natural for capturing such algebraic data structures.  This would generalize to any formal logic I can think of, making it sort of language neutral (before training).  However, I can also imagine one could use other serialization methods.  One could have used JSON or gRPC (which naturally translates to JSON) as the serialization method as well.  <span class="user-mention" data-user-id="246156">@Brando Miranda</span> has asked about JSON and said that Emilio at <a href="https://github.com/ejgallego/coq-serapi" target="_blank" title="https://github.com/ejgallego/coq-serapi">Serapi/Coq</a> was thinking about switching from s-expressions to JSON.  In some sense I don't think it matters much here as long as one uses a lossless encoding which is easy to parse.  (And in this case, these s-expressions are extremely easy to parse into whatever form one wants.)</li>
<li>They are also used as the text entered into the neural network model.  If one is using a pure sequence input model like an RNN or wavenet, then I think the idea is that you would plug the s-expression in as is (except one-hot encoding every token first?).  Or if one was using a tree or graph NN, then one naturally parses the s-expression into that tree or graph.  <span class="user-mention" data-user-id="249373">@Stanislas Polu</span> has lamented that the current s-expressions are probably too long for an RNN and has suggested shorter encodings.  Again, I think one has full freedom to play around with other representations.  I can think of many.  On the most compact side is to use the compact representation from the HOLStep data set.  It throws away types.  It uses polish notation.  And it uses skolemization and DeBruin indices for quantifiers and variables.  This however might be too compact.  HOList has found (from taking with Markus) that variable names matter a lot.  Another compact-ish approach would be to simulate the HOL Light pretty printer, but maybe add a few extra things.  It wouldn't be hard to get the parentheses the same as HOL Light.  As for types, the HOL Light pretty printer throws them away, but I think one might want to keep them for quantifiers and lambdas only.  Now, intermediate goals might not have any quantifiers, but one could borrow notation from say Lean and write something like this for an intermediate goal <code>(n: nat), (m: nat) |- = (num_add m n) (num_add n m)</code>.  It is hard to know what would work best without experimentation.  I've suggested that rather than trying this all out on HOList, it might be better to experiment first with some of the different string representations with different neural network models on HOLStep first since it is an easier to train dataset.  We would be looking for something which is quick to run but also does well on the task.  Since it is so up in the air what a good string input is, I think it wouldn't make sense to encode it into the HOList/DeepMath interface.  Instead, s-expressions work nice as a loss-less encoding which could be tweaked into something else when needed.</li>
<li>Last, one is also using these s-expressions as some sort of semi-human-readable term representation.  This is needed for debugging and understanding the outputs.  Again, the programmer is free to clean up the formula a bit for debugging, but most of the current HOList printouts use these s-expressions.</li>
</ul>
<p>So in summary, s-expressions try to be everything to everyone.  While in practice they might fail at that, they are pretty easy to parse and turn into something else.  The only exception is that the HOL Light pretty printed expressions are a bit complicated to 100% reproduce, but one can come close.  If it is important to have the exact pretty printed expressions, one could build that as another server call into the gRPC interface.  Get me the pretty printed version of this s-expression.</p>

<a name="185607167"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185607167" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185607167">Markus Rabe (Jan 14 2020 at 15:36)</a>:</h4>
<p>The response to any apply tactic request should already include the pretty printed version of the terms (as well as the s-expression, of course). Also all the theorem in the theorem database should have a pretty printed field in their proto.</p>

<a name="185624371"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185624371" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185624371">Christian Szegedy (Jan 14 2020 at 18:29)</a>:</h4>
<p>Our first models were convolutional networks that take the  tokenized s-expression (with types) as input. We have removed the parenthesis as well, as it is redundant, but makes parsing of the expressions a bit easier (esp. for humans). </p>
<p>We have tried to train sequence models on the pretty printed output as well, but it yielded inferior results to the models taking s-expressions as input.</p>
<p>Our graph-neural networks uses subexpression-sharing (still containing) which makes the input significantly shorter</p>
<p>Using JSON would have had the disadvantage that we would have needed a JSON parser in our Google internal version of HOL Light, which would have required importing extra packages.</p>

<a name="185655886"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185655886" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185655886">Jason Rute (Jan 15 2020 at 00:17)</a>:</h4>
<blockquote>
<p>The response to any apply tactic request should already include the pretty printed version of the terms </p>
</blockquote>
<p>I didn’t see this behavior in <a href="https://github.com/jasonrute/holist-communication-example/blob/master/walkthrough_of_holist_api.ipynb" target="_blank" title="https://github.com/jasonrute/holist-communication-example/blob/master/walkthrough_of_holist_api.ipynb">my notebook</a>.  All the apply tactic calls only return the full s-expressions.  For example see cell 10.</p>

<a name="185746134"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185746134" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185746134">Patrick Massot (Jan 15 2020 at 21:28)</a>:</h4>
<p><span class="user-mention" data-user-id="110026">@Simon Hudon</span> are you following this thread in order to see how the Lean4 editor integration could also be a machine learning rig integration? Or is it something completely different?</p>

<a name="185746790"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185746790" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185746790">Jesse Michael Han (Jan 15 2020 at 21:35)</a>:</h4>
<p>such integration would be more structured than the current language server protocol, which does not serialize the environment, nor fully elaborated terms (and only does so as unstructured strings via JSON)</p>

<a name="185746915"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185746915" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185746915">Simon Hudon (Jan 15 2020 at 21:36)</a>:</h4>
<p>The next language server will serialize the syntax tree and the type information</p>

<a name="185747146"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185747146" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185747146">Simon Hudon (Jan 15 2020 at 21:38)</a>:</h4>
<p><span class="user-mention" data-user-id="110031">@Patrick Massot</span>, does that answer your question? I'm not sure I understood what you were looking for</p>

<a name="185747174"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185747174" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185747174">Simon Cruanes (Jan 15 2020 at 21:39)</a>:</h4>
<p>So it'll still be bespoke and not LSP based?</p>

<a name="185747579"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185747579" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185747579">Patrick Massot (Jan 15 2020 at 21:43)</a>:</h4>
<blockquote>
<p>So it'll still be bespoke and not LSP based?</p>
</blockquote>
<p>We can add as many extension to LSP as we want.</p>

<a name="185747586"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185747586" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185747586">Simon Hudon (Jan 15 2020 at 21:43)</a>:</h4>
<p>We're basing it on LSP but we're going to get beyond the LSP basic features for the more advance uses</p>

<a name="185747611"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185747611" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185747611">Patrick Massot (Jan 15 2020 at 21:43)</a>:</h4>
<blockquote>
<p>Patrick Massot, does that answer your question? I'm not sure I understood what you were looking for</p>
</blockquote>
<p>I have no idea. I only hope people who followed this thread will know.</p>

<a name="185747612"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185747612" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185747612">Mario Carneiro (Jan 15 2020 at 21:43)</a>:</h4>
<p>I'm hoping you document those extensions</p>

<a name="185747736"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185747736" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185747736">Simon Cruanes (Jan 15 2020 at 21:45)</a>:</h4>
<blockquote>
<p>We can add as many extension to LSP as we want.</p>
</blockquote>
<p>yes, but LSP remains based on buffers and JSON, I'm not sure I see how you can carry ASTs on it efficiently?</p>

<a name="185747750"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185747750" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185747750">Mario Carneiro (Jan 15 2020 at 21:45)</a>:</h4>
<p>you can json anything</p>

<a name="185747804"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185747804" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185747804">Mario Carneiro (Jan 15 2020 at 21:46)</a>:</h4>
<p>I guess the efficiency isn't so great, but as long as it's only sent when needed it shouldn't be so bad</p>

<a name="185747855"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185747855" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185747855">Simon Cruanes (Jan 15 2020 at 21:46)</a>:</h4>
<p>sometimes I wish LSP had been built on msgpack-rpc or something like that</p>

<a name="185748765"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185748765" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185748765">Mario Carneiro (Jan 15 2020 at 21:58)</a>:</h4>
<p>huh, msgpack is pretty cool</p>

<a name="185749163"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/185749163" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#185749163">Simon Cruanes (Jan 15 2020 at 22:03)</a>:</h4>
<p>especially when you want to embed big chunks of code into it… no escaping needed</p>

<a name="186201897"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186201897" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186201897">Stanislas Polu (Jan 21 2020 at 16:46)</a>:</h4>
<p><span class="user-mention" data-user-id="115715">@Jason Rute</span> and others friends knowledgeable about Holist. I've been able to spin up a holist instance using the <code>gcr.io/deepmath/hol-light</code> image. One thing that I expected from reading the code and introspecting the proof logs was that all theorems fingerprints appearing in the proof logs would be directly usable with that image but it looks like that's not the case. How does one is supposed to interact with the prover for a goal part of the test set? Do they have to replay and register all theorems first?</p>

<a name="186202542"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186202542" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186202542">Stanislas Polu (Jan 21 2020 at 16:53)</a>:</h4>
<p>Example code failing:</p>
<div class="codehilite"><pre><span></span>for_all_x_exists_y_x_equals_y = Theorem(
    name=&quot;FORALL_X_EXISTS_Y_SUCH_THAT_X_EQUALS_Y&quot;,
    conclusion=&quot;(a (c (fun (fun A (bool)) (bool)) !) (l (v A x) (a (c (fun (fun A (bool)) (bool)) ?) (l (v A y) (a (a (c (fun A (fun A (bool))) =) (v A x)) (v A y))))))&quot;,
    training_split=Theorem.Split.TESTING,
    tag=Theorem.Tag.THEOREM,
)

if __name__ == &#39;__main__&#39;:
    with grpc.insecure_channel(&#39;10.72.7.138:2000&#39;) as channel:
        stub = ProofAssistantServiceStub(channel)

        request3 = ApplyTacticRequest(goal=for_all_x_exists_y_x_equals_y, tactic=&quot;SIMP_TAC [ THM 220805353555668225 ]&quot;)
        print(&quot;Request:&quot;)
        print(request3)

        response3 = stub.ApplyTactic(request3)
        print(&quot;Response:&quot;)
        print(response3)
</pre></div>


<p>Where <code>220805353555668225</code> is the fingerprint of a theorem argument taken from the training set (appears in <code>human/train/prooflogs-00037-of-00600.pbtxt</code>)</p>
<p>This gives:</p>
<div class="codehilite"><pre><span></span>Request:
goal {
  conclusion: &quot;(a (c (fun (fun A (bool)) (bool)) !) (l (v A x) (a (c (fun (fun A (bool)) (bool)) ?) (l (v A y) (a (a (c (fun A (fun A (bool))) =) (v A x)) (v A y))))))&quot;
  tag: THEOREM
  name: &quot;FORALL_X_EXISTS_Y_SUCH_THAT_X_EQUALS_Y&quot;
  training_split: TESTING
}
tactic: &quot;SIMP_TAC [ THM 220805353555668225 ]&quot;

Response:
error: &quot;Failure(\&quot;No theorem exists with index 220805353555668225\&quot;)&quot;
</pre></div>

<a name="186202862"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186202862" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186202862">Stanislas Polu (Jan 21 2020 at 16:56)</a>:</h4>
<p>Or in other words how can I easily replay a proof log ?</p>

<a name="186203965"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186203965" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186203965">Stanislas Polu (Jan 21 2020 at 17:06)</a>:</h4>
<p>Ah I now realize that some theorems in <code>theorem_database_v1.1.textpb</code> are registered and usable.  I think only definitions are registered, but other theorems are not. </p>
<p>The question therefore remains?</p>

<a name="186210355"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186210355" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186210355">Jason Rute (Jan 21 2020 at 18:10)</a>:</h4>
<p>I was under the impression, possibly wrong, that one needs to replay (VerifyProof) and register (RegisterTheorem) all the theorems.  Of course this is assuming you are working in the “low level” gRPC API.  If you are working in the “high level” Python API then I assume the Python code does that stuff for you, but I haven’t explored that as much yet.</p>

<a name="186211833"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186211833" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186211833">Stanislas Polu (Jan 21 2020 at 18:27)</a>:</h4>
<p>Maybe <span class="user-mention" data-user-id="217806">@Markus Rabe</span> or <span class="user-mention" data-user-id="239426">@Christian Szegedy</span> can shed some light on this? <span aria-label="grimacing" class="emoji emoji-1f62c" role="img" title="grimacing">:grimacing:</span></p>

<a name="186218006"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186218006" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186218006">Christian Szegedy (Jan 21 2020 at 19:31)</a>:</h4>
<blockquote>
<blockquote>
<p>The response to any apply tactic request should already include the pretty printed version of the terms </p>
</blockquote>
<p>I didn’t see this behavior in <a href="https://github.com/jasonrute/holist-communication-example/blob/master/walkthrough_of_holist_api.ipynb" target="_blank" title="https://github.com/jasonrute/holist-communication-example/blob/master/walkthrough_of_holist_api.ipynb">my notebook</a>.  All the apply tactic calls only return the full s-expressions.  For example see cell 10.</p>
</blockquote>
<p>It looks like (unfortunately) that this is only in our Google-internal version. We could do another round of exporting, especially that we have some code for an ICLR paper that should be open-sourced as well.</p>

<a name="186218426"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186218426" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186218426">Christian Szegedy (Jan 21 2020 at 19:35)</a>:</h4>
<blockquote>
<p>I was under the impression, possibly wrong, that one needs to replay (VerifyProof) and register (RegisterTheorem) all the theorems.  Of course this is assuming you are working in the “low level” gRPC API.  If you are working in the “high level” Python API then I assume the Python code does that stuff for you, but I haven’t explored that as much yet.</p>
</blockquote>
<p>The current verifier verifies theorems in their original context. So currently. you need to replay the whole theorem library in order to verify any number of theorems:<br>
- You can verify only theorems that came from the HOL-Light library (complex),<br>
- You can verify any number of theorems (you don't need to verify all of them)<br>
- All the theorems in the library will be run through the kernel for verification.<br>
- Those theorems that had an external proof (to be verified) will use their external proof at exactly that position where the theorem was proved originally.</p>

<a name="186220118"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186220118" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186220118">Christian Szegedy (Jan 21 2020 at 19:52)</a>:</h4>
<blockquote>
<p>Ah I now realize that some theorems in <code>theorem_database_v1.1.textpb</code> are registered and usable.  I think only definitions are registered, but other theorems are not. </p>
<p>The question therefore remains?</p>
</blockquote>
<p>The human prooflogs contain proof-steps that rely on theorems created "on the fly" by forward reasoning steps (so called conversions). These theorems don't show up in the theorem database. Also proofs relying on them can't be replayed as conversions can't be replayed either.</p>
<p>On the other hand the exported tensorflow examples contain the actual s-expression of those parameters, so if somebody uses those examples, the corresponding theorem can be used for training the parameter-selection models, even if those tactic-parameters don't show up in the proof-logs.</p>
<p>Around 60% of the human proofs can be replayed, as a large number of them uses theorems deduced by forward reasoning steps, other theorems use ad-hoc substitution of terms that we did not log either.</p>

<a name="186252165"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186252165" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186252165">Markus Rabe (Jan 22 2020 at 03:20)</a>:</h4>
<p><span class="user-mention" data-user-id="249373">@Stanislas Polu</span> <span class="user-mention" data-user-id="115715">@Jason Rute</span><br>
The example above failed because the theorem with the fingerprint (or "index") 220805353555668225 is unknown to HOL Light when it is started up. Only the "core" Theorems are loaded at that point (which includes some basic definitions and theorems). Before you use any other theorem you need to register that theorem with the RegisterTheoremRequest.</p>
<p>As Christian said, don't expect all human-written proofs to go through, as some tactics are not supported and not all theorems that humans used in the proofs are in our theorem database.</p>
<p>As a final note: VerifyProof was only used for a deprecated version of our proof checker. Currently it is not used for anything and should be ignored.</p>

<a name="186268156"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186268156" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186268156">Stanislas Polu (Jan 22 2020 at 09:18)</a>:</h4>
<p><span class="user-mention" data-user-id="217806">@Markus Rabe</span> Thanks! Let me experiment with that. Ack re the obvious non-possibility to input any human proof. That being said, is it true that we can proof check all the proofs in the proof logs?</p>

<a name="186268720"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186268720" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186268720">Stanislas Polu (Jan 22 2020 at 09:26)</a>:</h4>
<p>Ah from reading <span class="user-mention" data-user-id="239426">@Christian Szegedy</span> post above I realize that property (replayability of the proof logs) does not hold true. If only 60% of the human proofs can be replayed, then 60% is somewhat of an upper bound on the performance of any supervised prover, right? (not sure it's mentioned in the papers?)</p>

<a name="186269417"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186269417" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186269417">Stanislas Polu (Jan 22 2020 at 09:35)</a>:</h4>
<blockquote>
<p>On the other hand the exported tensorflow examples contain the actual s-expression of those parameters, so if somebody uses those examples, the corresponding theorem can be used for training the parameter-selection models, even if those tactic-parameters don't show up in the proof-logs.</p>
</blockquote>
<p>I was under the impression that the information contained in the proof logs (pbtxt) was equivalent to the information contained in the tf examples (for the positive arguments) ? Is it not true? Is there a source of truth for the specification of the content of these files?</p>

<a name="186269695"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186269695" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186269695">Stanislas Polu (Jan 22 2020 at 09:39)</a>:</h4>
<p>Finally <span class="user-mention" data-user-id="217806">@Markus Rabe</span> is your latest comment on VerifyProof true of the publicly available version? (Thanks thanks!)</p>

<a name="186272422"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186272422" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186272422">Jason Rute (Jan 22 2020 at 10:14)</a>:</h4>
<p><span class="user-mention" data-user-id="217806">@Markus Rabe</span> I’m under the impression from my experiments with the public HOList Docker image that one has to first verify a theorem before one can register it (and it has to be the most recently verified theorem).  I assume this has gone away in the internal versions.</p>

<a name="186272506"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186272506" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186272506">Jason Rute (Jan 22 2020 at 10:15)</a>:</h4>
<p>Also, I assume the way that RegisterTheorem now works is to add the theorem as an axiom using the CHEAT tactic?</p>

<a name="186278286"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186278286" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186278286">Stanislas Polu (Jan 22 2020 at 11:39)</a>:</h4>
<p>Form the code in [0], it looks like anything received from RegisterTheorem is indeed assumed:</p>
<div class="codehilite"><pre><span></span>Theorem_fingerprint.index_thm index (Drule.mk_thm (to_term_list gs));
</pre></div>


<p>As Drule.mk_thm is a wrapper around the ASSUME tactic I believe.</p>
<p>But unclear if [0] is what is deployed in the <code>gcr.io/deepmath/hol-light</code> image?</p>
<p>[0] <a href="https://github.com/brain-research/hol-light/blob/master/sandboxee.ml#L141-L144" target="_blank" title="https://github.com/brain-research/hol-light/blob/master/sandboxee.ml#L141-L144">https://github.com/brain-research/hol-light/blob/master/sandboxee.ml#L141-L144</a></p>

<a name="186305426"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186305426" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186305426">Stanislas Polu (Jan 22 2020 at 16:46)</a>:</h4>
<p>I confirm that you can register any theorem (Only caveat is that it seems that the fingerprint does not get returned if not passed).</p>

<a name="186305605"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186305605" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186305605">Stanislas Polu (Jan 22 2020 at 16:48)</a>:</h4>
<p><span class="user-mention" data-user-id="239426">@Christian Szegedy</span> can you expand on the limitation related to conversions? Replaying the prooflogs would just fail there? From skimming through the <a href="http://parse_tactic.ml" target="_blank" title="http://parse_tactic.ml">parse_tactic.ml</a> code it looks like there is some support for conversions? What are the current limitations?</p>

<a name="186329092"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186329092" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186329092">Christian Szegedy (Jan 22 2020 at 20:45)</a>:</h4>
<blockquote>
<p>If only 60% of the human proofs can be replayed, then 60% is somewhat of an upper bound on the performance of any supervised prover, right? (not sure it's mentioned in the papers?)</p>
</blockquote>
<p>This is not necessarily true. Since we use the network to guide a search, if the search explores enough branches it can discover proofs that are alternative to the human proofs.</p>

<a name="186329489"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186329489" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186329489">Christian Szegedy (Jan 22 2020 at 20:50)</a>:</h4>
<blockquote>
<p><span class="user-mention silent" data-user-id="239426">Christian Szegedy</span> can you expand on the limitation related to conversions? Replaying the prooflogs would just fail there? From skimming through the <a href="http://parse_tactic.ml" target="_blank" title="http://parse_tactic.ml">parse_tactic.ml</a> code it looks like there is some support for conversions? What are the current limitations?</p>
</blockquote>
<p>We log some of the conversions, but the ApplyTactic function call can't make use of them.</p>
<p>We have a new internal extension to the API that allows the application of "rules" (forward reasoning steps), we have run some preliminary expriments using them, but we don't have a proper integration, especially proof replay and verification are not implemented for them. We plan to open source our extensions to the interface as soon as there is enough interest in the community to use it.</p>

<a name="186330963"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186330963" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186330963">Christian Szegedy (Jan 22 2020 at 21:06)</a>:</h4>
<p>BTW, this is the code to properly initialize the environment with all of the (complex) proofs:</p>
<p><a href="https://github.com/tensorflow/deepmath/blob/c51df033cdf8d2d103fd277beb3b9acf39b8d9c1/deepmath/deephol/prover.py#L359" target="_blank" title="https://github.com/tensorflow/deepmath/blob/c51df033cdf8d2d103fd277beb3b9acf39b8d9c1/deepmath/deephol/prover.py#L359">https://github.com/tensorflow/deepmath/blob/c51df033cdf8d2d103fd277beb3b9acf39b8d9c1/deepmath/deephol/prover.py#L359</a></p>

<a name="186332996"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186332996" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186332996">Markus Rabe (Jan 22 2020 at 21:30)</a>:</h4>
<p><span class="user-mention" data-user-id="249373">@Stanislas Polu</span> You're right. The publicly available version of our modified HOL Light still includes the old proof checker. It should still work. <br>
What I meant is that we have a new proof checker that bypasses the API and instead compiles proof logs to OCaml code. Thereby we don't have to trust our Python code and the API. The new proof checker is described at <a href="http://deephol.org" target="_blank" title="http://deephol.org">deephol.org</a>.</p>

<a name="186333304"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186333304" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186333304">Markus Rabe (Jan 22 2020 at 21:33)</a>:</h4>
<p><span class="user-mention" data-user-id="115715">@Jason Rute</span> <span class="user-mention" data-user-id="249373">@Stanislas Polu</span> Indeed, you can even register terms that are not true. So it is your responsibility to stay sound in the stateless API. (Hence our stand-alone proof checker.)</p>

<a name="186366816"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186366816" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186366816">Stanislas Polu (Jan 23 2020 at 08:18)</a>:</h4>
<p><span class="user-mention" data-user-id="239426">@Christian Szegedy</span>  Thanks for your additional comments</p>

<a name="186366828"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186366828" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186366828">Stanislas Polu (Jan 23 2020 at 08:18)</a>:</h4>
<blockquote>
<p>BTW, this is the code to properly initialize the environment with all of the (complex) proofs:<br>
<a href="https://github.com/tensorflow/deepmath/blob/c51df033cdf8d2d103fd277beb3b9acf39b8d9c1/deepmath/deephol/prover.py#L359" target="_blank" title="https://github.com/tensorflow/deepmath/blob/c51df033cdf8d2d103fd277beb3b9acf39b8d9c1/deepmath/deephol/prover.py#L359">https://github.com/tensorflow/deepmath/blob/c51df033cdf8d2d103fd277beb3b9acf39b8d9c1/deepmath/deephol/prover.py#L359</a></p>
</blockquote>
<p>I had stumbled on this and was plan to use exactly that <span aria-label="+1" class="emoji emoji-1f44d" role="img" title="+1">:+1:</span></p>
<blockquote>
<p>We have a new internal extension to the API that allows the application of "rules" (forward reasoning steps), we have run some preliminary expriments using them, but we don't have a proper integration, especially proof replay and verification are not implemented for them. We plan to open source our extensions to the interface as soon as there is enough interest in the community to use it.</p>
</blockquote>
<p>There is interest to use it :)</p>

<a name="186366915"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186366915" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186366915">Stanislas Polu (Jan 23 2020 at 08:20)</a>:</h4>
<p><span class="user-mention" data-user-id="217806">@Markus Rabe</span> thanks!</p>

<a name="186367435"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186367435" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186367435">Stanislas Polu (Jan 23 2020 at 08:30)</a>:</h4>
<p><span class="user-mention" data-user-id="239426">@Christian Szegedy</span> Let me rephrase a bit what you stated. Even without support for forward reasoning with conversions one could imagine that a system that takes the results of the conversion (input to later tactics) as part of its training set, it could potentially replay some theorems successfully skipping entirely the conv step. Do you guys include these theorems to your database when training?</p>

<a name="186409482"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186409482" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186409482">Christian Szegedy (Jan 23 2020 at 16:56)</a>:</h4>
<p><span class="user-mention" data-user-id="249373">@Stanislas Polu</span> If you mean by "replay" proving the theorem in a different way, then yes. It happens a lot. Actually, machine generated proofs tend to be quite different from human proofs, even if premise selection was trained by imitation only. Also logging (and training on) "theorems" (or true statements) produced by the conversions is still useful for training the models that decides which tactic parameters are the most useful, even if the prover process cannot do conversions. That's what we do.</p>

<a name="186421586"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186421586" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186421586">Stanislas Polu (Jan 23 2020 at 18:52)</a>:</h4>
<p><span class="user-mention" data-user-id="239426">@Christian Szegedy</span> thanks!</p>

<a name="186476176"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186476176" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186476176">Stanislas Polu (Jan 24 2020 at 09:43)</a>:</h4>
<p><span class="user-mention" data-user-id="217806">@Markus Rabe</span> I'm a bit surprised by the following:</p>
<div class="codehilite"><pre><span></span>root@dev-1-0:~# du -h ~/deephol-data/deepmath/deephol/proofs/human/test
3.3G    /root/deephol-data/deepmath/deephol/proofs/human/test
root@dev-1-0:~# du -h ~/deephol-data/deepmath/deephol/proofs/human/valid
2.4G    /root/deephol-data/deepmath/deephol/proofs/human/valid
root@dev-1-0:~# du -h ~/deephol-data/deepmath/deephol/proofs/human/train
9.2G    /root/deephol-data/deepmath/deephol/proofs/human/train
</pre></div>


<p>vs</p>
<div class="codehilite"><pre><span></span>root@dev-1-0:~# grep &quot;TEST&quot; ~/deephol-data/deepmath/deephol/theorem_database_v1.1.textpb | wc -l
14141
root@dev-1-0:~# grep &quot;VALIDATION&quot; ~/deephol-data/deepmath/deephol/theorem_database_v1.1.textpb | wc -l
3668
root@dev-1-0:~# grep &quot;TRAINING&quot; ~/deephol-data/deepmath/deephol/theorem_database_v1.1.textpb | wc -l
11655
</pre></div>


<p>Which gives a bytes/theorem (approximation) of:</p>
<div class="codehilite"><pre><span></span>TEST: ~233k
VALIDATION: ~654k
TRAIN: ~789k
</pre></div>


<p>Which indicates that the size of the proofs in the test set are noticeably smaller vs train/validation? Is that intentional? (or maybe a discrepancy in tf records for the test set?)</p>

<a name="186487969"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186487969" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186487969">Stanislas Polu (Jan 24 2020 at 12:46)</a>:</h4>
<p><span class="user-mention" data-user-id="217806">@Markus Rabe</span> sorry another quick question. It looks like <code>LABEL_TAC</code> is not accepted by the proof assistant. Any reason why they appear in the proof logs?</p>

<a name="186491122"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186491122" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186491122">Stanislas Polu (Jan 24 2020 at 13:25)</a>:</h4>
<p>FInally, while attempting to replay the proof logs with the following procedure: for all tactics taking a theorem argument, call RegisterTheorem on each argument and then for all tactics, call ApplyTactic.</p>
<p>I often, despite a successful call to RegisterTheorem (the fingerprint I use (the one from the proof logs) is returned to me with no error), I get an eventual error when applying the tactic with message "No theorem exists with index XXX". From reading the ML code, I have a hard time seeing how the fingerprint can be returned successfully at RegisterTheorem and then not being hitting that message meaning that the index does not has that fingerprint.</p>
<p>Example logs:</p>
<div class="codehilite"><pre><span></span>Replaying /Users/spolu/deephol-data/deepmath/deephol/proofs/human/test/prooflogs-00218-of-00537.jsonl
&gt; ApplyTactic X_GEN_TAC `(v (fun (cart (real) N) (bool)) s)`
&lt; ApplyTactic 1 goals
&gt; ApplyTactic X_GEN_TAC `(v (fun (cart (real) N) (bool)) t)`
&lt; ApplyTactic 1 goals
&gt; ApplyTactic DISCH_TAC
&lt; ApplyTactic 1 goals
&gt; ApplyTactic RAW_POP_TAC 0
&lt; ApplyTactic 1 goals
&gt; RegisterTheorem 3261843337692443473
&lt; RegisterTheorem 3261843337692443473
&gt; ApplyTactic REWRITE_TAC [ THM 3261843337692443473 ]
&lt; ApplyTactic 1 goals
&gt; RegisterTheorem 3276577123099513888
&lt; RegisterTheorem 3276577123099513888
&gt; ApplyTactic MP_TAC THM 3276577123099513888
!!! Failure(&quot;No theorem exists with index 3276577123099513888&quot;)
&gt; RegisterTheorem 3550116108290505704
&lt; RegisterTheorem 3550116108290505704
&gt; RegisterTheorem 1457252911920550478
&lt; RegisterTheorem 1457252911920550478
&gt; RegisterTheorem 4032148915606337071
&lt; RegisterTheorem 4032148915606337071
&gt; RegisterTheorem 1358160238249633387
&lt; RegisterTheorem 1358160238249633387
&gt; RegisterTheorem 34540287283234494
&lt; RegisterTheorem 34540287283234494
&gt; RegisterTheorem 3806712188414811372
&lt; RegisterTheorem 3806712188414811372
&gt; ApplyTactic SIMP_TAC [ THM 3550116108290505704 ; THM 1457252911920550478 ; THM 4032148915606337071 ; THM 1358160238249633387 ; THM 34540287283234494 ; THM 3806712188414811372 ]
!!! Failure(&quot;No theorem exists with index 34540287283234494&quot;)
&gt; ApplyTactic DISCH_TAC
&lt; ApplyTactic 1 goals
&gt; ApplyTactic RAW_POP_TAC 0
&lt; ApplyTactic 1 goals
&gt; RegisterTheorem 4441231563045595206
&lt; RegisterTheorem 4441231563045595206
&gt; ApplyTactic MP_TAC THM 4441231563045595206
!!! Failure(&quot;No theorem exists with index 4441231563045595206&quot;)
&gt; ApplyTactic ANTS_TAC
&lt; ApplyTactic 2 goals
&gt; RegisterTheorem 647530560333096154
&lt; RegisterTheorem 647530560333096154
&gt; ApplyTactic REWRITE_TAC [ THM 647530560333096154 ]
&lt; ApplyTactic 1 goals
</pre></div>


<p>As you can see this happens quite often.<br>
cc <span class="user-mention" data-user-id="217806">@Markus Rabe</span> <span class="user-mention" data-user-id="239426">@Christian Szegedy</span></p>

<a name="186491176"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186491176" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186491176">Stanislas Polu (Jan 24 2020 at 13:26)</a>:</h4>
<p>(sorry for all the questions, hope it'll be useful to the community)</p>

<a name="186495853"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186495853" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186495853">Stanislas Polu (Jan 24 2020 at 14:22)</a>:</h4>
<p>I believe I found what is going on from inspecting the logs of the container. It seems that the fingerprint mismatch. Unfortunately the "api" does not return the newly computed fingerprint but only the fingerprint passer as argument, but does use the new fingerprint for indexing. This is a bit unfortunate.</p>

<a name="186495865"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186495865" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186495865">Stanislas Polu (Jan 24 2020 at 14:22)</a>:</h4>
<p>(I'll explore as to why there are discrepancies in fingerprints)</p>

<a name="186498505"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/HOList/near/186498505" class="zl"><img src="https://leanprover-community.github.io/assets/img/zulip2.png" alt="view this post on Zulip"></a> <a href="https://leanprover-community.github.io/archive/219941MachineLearningforTheoremProving/34292HOList.html#186498505">Stanislas Polu (Jan 24 2020 at 14:54)</a>:</h4>
<p>Problem solved on my end by fixing discrepancies in the S-Expr I was sending (escaped characters were not handled properly). Replaying logs seems to work properly now \o/ Hope this is all useful to others.</p>


{% endraw %}

{% include archive_update.html %}